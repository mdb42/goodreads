@inproceedings{wan2018,
  author    = {Wan, Mengting and McAuley, Julian},
  title     = {Item Recommendation on Monotonic Behavior Chains},
  booktitle = {Proceedings of the 12th ACM Conference on Recommender Systems},
  series    = {RecSys '18},
  pages     = {86--94},
  year      = {2018},
  publisher = {Association for Computing Machinery},
  doi       = {10.1145/3240323.3240369}
}

@inproceedings{wan2019,
  author    = {Wan, Mengting and Misra, Rishabh and Nakashole, Ndapa and McAuley, Julian},
  title     = {Fine-Grained Spoiler Detection from Large-Scale Review Corpora},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages     = {2605--2610},
  year      = {2019},
  doi       = {10.18653/v1/P19-1248}
}

@article{monika2023,
  author    = {Monika, L. and Chooralil, V.S.},
  title     = {Sentiment Analysis: A Survey on Design Framework, Applications and Future Scopes},
  journal   = {Artificial Intelligence Review},
  volume    = {56},
  pages     = {12505--12560},
  year      = {2023},
  doi       = {10.1007/s10462-023-10442-2}
}

@book{reagle2015,
  author    = {Reagle, Joseph M.},
  title     = {Reading the Comments: Likers, Haters, and Manipulators at the Bottom of the Web},
  publisher = {MIT Press},
  year      = {2015}
}

@book{barekat2017,
  author    = {Barekat, Houman and Barry, Robert and Winters, David},
  title     = {The Digital Critic: Literary Culture Online},
  publisher = {OR Books},
  year      = {2017},
  address   = {New York}
}

@inproceedings{chandra2020,
	author={Chandra, Yogesh and Jana, Antoreep},
	booktitle={2020 7th International Conference on Computing for Sustainable Global Development (INDIACom)}, 
	title={Sentiment Analysis using Machine Learning and Deep Learning}, 
	year={2020},
	volume={},
	number={},
	pages={1-4},
	keywords={Deep learning;Sentiment analysis;Analytical models;Social networking (online);Computational modeling;Blogs;Government;twitter;sentiment analysis;polarity;machine learning;deep learning;LSTM;CNN},
	doi={10.23919/INDIACom49435.2020.9083703}}

@article{Elkins2019,
	title={Can Sentiment Analysis Reveal Structure in a Plotless Novel?},
	author={Kathrine Elkins and Jon Chun},
	journal={ArXiv},
	year={2019},
	volume={abs/1910.01441},
	url={https://api.semanticscholar.org/CorpusID:203641988}
}

@article{Sokolova2020,
	author       = {Marina Sokolova Victoria Bobicev},
	title        = {Machine Learning Evaluation of the Echo-Chamber Effect in Medical
	Forums},
	journal      = {CoRR},
	volume       = {abs/2010.09574},
	year         = {2020},
	url          = {https://arxiv.org/abs/2010.09574},
	eprinttype    = {arXiv},
	eprint       = {2010.09574},
	timestamp    = {Wed, 21 Oct 2020 12:11:48 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2010-09574.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zhou2020,
	title = "{S}enti{X}: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis",
	author = "Zhou, Jie  and
	Tian, Junfeng  and
	Wang, Rui  and
	Wu, Yuanbin  and
	Xiao, Wenming  and
	He, Liang",
	editor = "Scott, Donia  and
	Bel, Nuria  and
	Zong, Chengqing",
	booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
	month = dec,
	year = "2020",
	address = "Barcelona, Spain (Online)",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2020.coling-main.49/",
	doi = "10.18653/v1/2020.coling-main.49",
	pages = "568--579",
	abstract = "Pre-trained language models have been widely applied to cross-domain NLP tasks like sentiment analysis, achieving state-of-the-art performance. However, due to the variety of users' emotional expressions across domains, fine-tuning the pre-trained models on the source domain tends to overfit, leading to inferior results on the target domain. In this paper, we pre-train a sentiment-aware language model (SentiX) via domain-invariant sentiment knowledge from large-scale review datasets, and utilize it for cross-domain sentiment analysis task without fine-tuning. We propose several pre-training tasks based on existing lexicons and annotations at both token and sentence levels, such as emoticons, sentiment words, and ratings, without human interference. A series of experiments are conducted and the results indicate the great advantages of our model. We obtain new state-of-the-art results in all the cross-domain sentiment analysis tasks, and our proposed SentiX can be trained with only 1{\%} samples (18 samples) and it achieves better performance than BERT with 90{\%} samples."
}

@article{Quadri2020,
	author = {Quadri, Mir and Selvakumar, R.},
	year = {2020},
	month = {12},
	pages = {64-68},
	title = {Performance of Naïve Bayes in Sentiment Analysis of User Reviews Online},
	volume = {10},
	journal = {International Journal of Innovative Technology and Exploring Engineering},
	doi = {10.35940/ijitee.A8198.1210220}
}

@article{Hung2015,
	author = {Hung, Lai and Alfred, Rayner and Hijazi, Mohd},
	year = {2015},
	month = {10},
	pages = {2952-2956},
	title = {A Review on Feature Selection Methods for Sentiment Analysis},
	volume = {21},
	journal = {Advanced Science Letters},
	doi = {10.1166/asl.2015.6475}
}

@article{hajibayova2019,
  author  = {Hajibayova, Lala},
  title   = {Investigation of Goodreads’ Reviews: Kakutanied, Deceived or Simply Honest?},
  journal = {ResearchGate},
  year    = {2019},
  note    = {Available at: \url{https://www.researchgate.net/publication/331384350}},
}

@article{shahsavari2020,
  author    = {Shahsavari, Shadi and Ebrahimzadeh, Ehsan and Shahbazi, Baharak and Falahi, Maryam and Holur, Prashant and Bandari, Ramin and Tangherlini, Timothy R. and Roychowdhury, Vwani},
  title     = {An Automated Pipeline for Character and Relationship Extraction from Readers' Literary Book Reviews on Goodreads.com},
  journal   = {arXiv preprint},
  volume    = {arXiv:2004.09601},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.09601},
}
