\begin{table}[h]
	\renewcommand{\arraystretch}{1.3}
	\caption{Classification Performance Metrics}
	\label{tab:classification_metrics}
	\centering
	\begin{tabular}{l r}
		\hline
		\textbf{Metric} & \textbf{Value} \\
		\hline
		Accuracy  & 0.6074 \\
		Precision & 0.6359 \\
		Recall    & 0.4954 \\
		\(F_1\) Score  & 0.5173 \\
		\hline
	\end{tabular}
\end{table}

\autoref{tab:classification_metrics} displays the aggregated accuracy, precision, recall, and \(F_1\) scores across all classes when classifying a set of 120,000 reviews.
Both metrics for accuracy and precision returned at or above 60\%, while recall and \(F_1\) returned closer to 50\%.
While our model is able to determine the correct label for a review over 50\% of the time,
this demonstrates that only using a multinomial Na\"ive Bayes classifier may not be sufficient enough for certain contexts where
the overall accuracy of the model is prioritized.

\autoref{fig:cm} displays a confusion matrix depicting the effectiveness of our model's multinomial Na\"ive Bayes classifier over a set of 120,000 user reviews from Goodreads.
At a glance, this confusion matrix seems to suggest that a sentiment analysis using multinomial Na\"ive Bayes is effective when labeling strongly-rated reviews.
However, this seems to lose a bit of fidelity when more lukewarm reviews are processed, particularly for labels of 2-, 3-, and 4-star ratings. This indicates that the classifier has a bit more difficulty in determining which terms belong to a review that's expressing a moderated sentiment.

\begin{table*}[!t]
	\renewcommand{\arraystretch}{1.3}
	\caption{Summary of User Clusters}
	\label{tab:user_clusters}
	\centering
	\begin{tabular}{c r r r r r}
		\hline
		\textbf{Cluster} & \textbf{Size (Users)} & \textbf{Avg. Word Length} & \textbf{Avg. Rating} & \textbf{Num Reviews} & \textbf{Rating Variance} \\
		\hline
		0 & 779  & 3442.32 & 3.7354 & 7.7895 & 0.6028 \\
		1 & 4712 & 1104.07 & 3.7789 & 8.8326 & 0.6937 \\
		2 & 2304 & 2055.78 & 3.7747 & 9.1563 & 0.6473 \\
		3 & 7483 &  416.56 & 3.8417 & 6.7879 & 0.5923 \\
		4 & 116  & 6322.63 & 3.4145 & 3.6207 & 0.3552 \\
		\hline
	\end{tabular}
\end{table*}

\autoref{tab:user_clusters} shows the results of the K-means cluster analysis of Goodreads reviewers, which gives us insight into what archetypes may exist among reviewers.

Reveiwers were ultimately separated into four clusters. Cluster 0 seems to encapsulate fairly verbose reviewers with lower ratings relative to the rest of the clusters. Cluster 4 seems to contain the most verbose reviewers within the dataset. This
seems to suggest that more verbose reviewers tend to leave more negative comments relative to other reviewers. While the middle clusters contain the grand majority of reviewers in the dataset, it seems that these reviewers are much less likely to post long, negative reviews

\autoref{fig:pca} exhibits a principle components analysis visualization of the clustering analysis, which better demonstrates the size of each cluster and their exhibited variance. Consistent with the metrics shown in \autoref{tab:user_clusters},
we can see that both cluster 0 and 4 are the furthest right along the PC1
axis, indicating that these clusters contain much larger reviews. The remaining clusters
also have a greater variance along the PC2 axis, indicating that these reviewers are, in fact, leaving more positive reviews than those leaving longer reviews in general.