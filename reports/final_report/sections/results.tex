\begin{table}[h]
	\renewcommand{\arraystretch}{1.3}
	\caption{Classification Performance Metrics}
	\label{tab:classification_metrics}
	\centering
	\begin{tabular}{l r}
		\hline
		\textbf{Metric} & \textbf{Value} \\
		\hline
		Accuracy  & 0.6074 \\
		Precision & 0.6359 \\
		Recall    & 0.4954 \\
		\(F_1\) Score  & 0.5173 \\
		\hline
	\end{tabular}
\end{table}

\autoref{tab:classification_metrics} displays the aggregated accuracy, precision, recall, and \(F_1\) scores across all classes when classifying a set of 120,000 reviews.
Both metrics for accuracy and precision returned at or above 60\%, while recall and \(F_1\) returned closer to 50\%.
While our model is able to determine the correct label for a review over 50\% of the time,
this demonstrates that only using a multinomial Na\"ive Bayes classifier may not be sufficient enough for certain contexts where
effectiveness matters more.

\autoref{fig:cm} displays a confusion matrix depicting the effectiveness of our model's multinomial Na\"ive Bayes classifier over a set of 120,000 user reviews from Goodreads.
At a glance, this confusion matrix seems to suggest that a sentiment analysis using multinomial Na\"ive Bayes is effective when labeling strongly-rated reviews.
However, this seems to lose a bit of fidelity when more lukewarm reviews are processed, particularly for labels of 2-, 3-, and 4-star ratings. This indicates that the classifier has a bit more difficulty in determining which terms belong to a review that's expressing a moderated sentiment.

\begin{table*}[!t]
	\renewcommand{\arraystretch}{1.3}
	\caption{Summary of User Clusters}
	\label{tab:user_clusters}
	\centering
	\begin{tabular}{c r r r r r}
		\hline
		\textbf{Cluster} & \textbf{Size (Users)} & \textbf{Avg. Length} & \textbf{Avg. Rating} & \textbf{Num Reviews} & \textbf{Rating Variance} \\
		\hline
		0 & 779  & 3442.32 & 3.7354 & 7.7895 & 0.6028 \\
		1 & 4712 & 1104.07 & 3.7789 & 8.8326 & 0.6937 \\
		2 & 2304 & 2055.78 & 3.7747 & 9.1563 & 0.6473 \\
		3 & 7483 &  416.56 & 3.8417 & 6.7879 & 0.5923 \\
		4 & 116  & 6322.63 & 3.4145 & 3.6207 & 0.3552 \\
		\hline
	\end{tabular}
\end{table*}

\autoref{tab:user_clusters} shows the results of the cluster analysis of Goodreads reviewers, which gives us insight into the tendencies that these reveiwers exhibit.

This was separated into four clusters. Cluster 0 seems to encapsulate fairly verbose reviewers with lower ratings relative to the rest of the clusters. Cluster 4 seems to contain the most verbose reviewers within the dataset. This
seems to suggest that more verbose reviewers tend to leave more negative comments relative to other reviewers. While the middle clusters contain the grand majority of reviewers in the dataset, it seems that these reviewers are much less likely to post long, negative reviews

\autoref{fig:pca} exhibits a PCA visualization of the clustering analysis.